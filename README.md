# Silent Talk: Bridging Communication for Deaf and Blind

### Team Members
- **Vraj Patel** (21BECE30244)  
- **Harsh Patel** (21BECE30191)  
- **Vishva Patel** (21BECE30242)  
- **Charmi Rupareliya** (21BECE30278)

**Project Mentor:** Dr. Hitesh Patel  
**Department of Computer Engineering**

---

## 🧐 Introduction

**Silent Talk** is a pioneering AI/ML-powered platform designed to bridge the communication gap between **deaf** and **blind** individuals.  
Using advanced **sign language translation** into **audio** and **Braille**, Silent Talk enables seamless interactions, fosters inclusivity, and breaks sensory communication barriers.

Silent Talk is **not just a tool** — it's a vision for a more connected and accessible world.

---

## 🚀 Problem Approach

| Step | Description |
|:---|:---|
| 1. Communication Challenges | Deaf individuals rely on visual cues, while blind individuals depend on audio or tactile feedback — creating a major communication gap. |
| 2. Current Limitations | Existing solutions often cater to either deaf **or** blind communities, but rarely both together. |
| 3. Our Solution | Silent Talk translates **sign language** into **audio** (for blind users) and **Braille** (for deafblind communication), enabling two-way conversations. |

---

## 🛠️ Tools & Technologies

- **Programming Languages:** Python, JavaScript (if applicable)
- **Machine Learning Models:** CNN, LSTM
- **Datasets:** 
  - American Sign Language (ASL) Dataset
  - Indian Sign Language (ISL) Dataset
  - Word-Level American Sign Language (WLASL) Dataset
- **Evaluation Metrics:** Accuracy, Overfitting Analysis, Real-World Testing

---

## 🏗️ System Architecture

- **Input:** Visual Capture of Sign Language Gestures
- **Processing:** 
  - CNN/LSTM-based recognition
  - Sign-to-Text conversion
- **Output:** 
  - Audio feedback
  - Braille output (via hardware interface or simulated devices)

---

## 📊 Model Performance

| Dataset | Model | Accuracy | Remarks |
|:---|:---|:---|:---|
| ASL | CNN | 99% | Overfitting observed |
| ISL | CNN | 99% | Overfitting observed |
| ISL | LSTM | 99% | Overfitting observed |
| WLASL | LSTM | 72% | Good accuracy |
| WLASL | CNN | 65% | Moderate accuracy |
| ASL | LSTM | 89% | Best balanced fit |

---

## ✨ Key Features

- **Real-time Translation:** Instant conversion of sign language to audio/Braille.
- **Customization:** Personalized settings based on user preferences.
- **Portability:** Compact design for use across various environments.
- **Scalability:** Suitable for schools, workplaces, public services, and more.

---

## 🎯 Project Goals

- **Primary Goals:**  
  Build a robust, high-accuracy communication system that's user-friendly and reliable.

- **Secondary Goals:**  
  Continuously refine datasets, enhance model capabilities, and collaborate with NGOs and organizations for real-world deployment.

- **Long-Term Vision:**  
  Foster a truly inclusive society where sensory limitations no longer limit communication.

---

## 🛠️ Current Modules

- Sign Language Detection
- Audio Conversion
- Braille Output
- Customization Panel
- Real-time Feedback Loop

---

## 🛸 Future Work

- Improve generalization across various sign languages globally (e.g., BSL, ISL variants)
- Minimize overfitting via advanced regularization techniques
- Develop low-cost, real-time Braille output devices
- Expand multi-language support
- Launch pilot programs with accessibility organizations

---

## 📚 References

1. ISL Dataset  
2. WLASL Dataset  
3. ASL Dataset  
4. Research papers and frameworks in Sign Language Recognition

---

> **Silent Talk: Empowering Connections, One Gesture at a Time.** 🌟

